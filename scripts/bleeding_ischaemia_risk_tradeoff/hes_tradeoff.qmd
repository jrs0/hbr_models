---
title: "Bleeding/Ischaemia Risk Trade-off Models"
subtitle: "Using HES data"
date: now
date-format: short
author: John Scott
project:
  output-dir: _output
execute:
  echo: false
  warning: false
  error: true
format:
  html: default
  pdf:
    keep-tex: true
jupyter: python3
engine: jupyter
fig-format: png
bibliography: references.bib
csl: citation_style.csl
geometry:
  - top=15mm
  - left=15mm
  - heightrounded
---

```{python}
# Some tips on using this file
# - print() does not work for rendering markdown in code block. It will mess up
#   the order of figures and text (e.g. headings)
# - To make heading, using display(Markdown("\nHeading Name")). The newline is
#   important, and I coulnd't figure out any other way other than \n to add it.
# - In order to use inline variables like `{python} variable`, you need at least 
#   quarto 1.4. You can get the prerelease from here: https://quarto.org/docs/download/prerelease
#   (it doesn't require admin rights).
#   

import os

os.chdir("../prototypes")

import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display, Markdown

import save_datasets as ds
from stability import plot_instability, draw_experiment_plan
from calibration import (
    get_bootstrapped_calibration,
    plot_calibration_curves,
    plot_prediction_distribution,
)
from summarise_model import get_model_summary, plot_model_validation_2page, plot_roc_and_calibration_2x2, plot_instability_2x2, plot_risk_tradeoff
from roc import get_bootstrapped_roc, get_bootstrapped_auc, plot_roc_curves

# The dictionaries map the identifier used in filenames and in the program
# to the string that will be printed in the document. Use .title() to convert
# to a heading-like version.
dataset_name = {
    "manual_codes": "HES (manual code groups)",
    "all_codes": "HES (all codes)",
    "manual_codes_swd": "HES (manual code groups) + SWD",
}
model_names = {
    ### HES (manual_codes) models ###
    "simple_logistic_regression": "logistic regression",
    "truncsvd_logistic_regression": "truncated SVD + logistic regression",
    #"simple_decision_tree": "decision tree",
    "truncsvd_decision_tree": "truncated SVD + decision tree",
    "simple_gradient_boosted_tree": "gradient boosted decision tree",
    "simple_random_forest": "random forest",
    "simple_naive_bayes": "naive bayes",
    "simple_neural_network": "single-layer neural network",
    ### HES + SWD models ###
    # "simple_logistic_regression": "logistic regression",
    # "truncsvd_logistic_regression": "truncated SVD + logistic regression",
    # "simple_decision_tree": "decision tree",
    # "truncsvd_decision_tree": "truncated SVD + decision tree",
    # "simple_gradient_boosted_tree": "gradient boosted decision tree",
    # "simple_random_forest": "random forest",
    # "simple_naive_bayes": "naive bayes",
    # #"simple_neural_network": "single-layer neural network",
    ### HES + SWD models ###  
    # #"simple_logistic_regression": "logistic regression",
    # "truncsvd_logistic_regression": "truncated SVD + logistic regression",
    # "simple_decision_tree": "decision tree",
    # "truncsvd_decision_tree": "truncated SVD + decision tree",
    # #"simple_gradient_boosted_tree": "gradient boosted decision tree",
    # #"simple_random_forest": "random forest",
    # #"simple_naive_bayes": "naive bayes",
    # #"simple_neural_network": "single-layer neural network",    
}
outcome_names = {
    "bleeding_al_ani_outcome": "bleeding",
    "hussain_ami_stroke_outcome": "AMI or stroke",
}

# The generated report relates to one dataset. Pick that dataset here.
dataset = "manual_codes"
dataset_title = dataset_name[dataset]
```

# Summary

```{python}
#| output: false

# Load any model to find which dataset was used (the script assumes that the same dataset
# was used to fit all models)
example_model_info = ds.load_fit_info(f"{dataset}_{list(model_names)[0]}_{list(outcome_names)[0]}")
dataset_path = example_model_info["dataset_path"]
data = pd.read_pickle(dataset_path)

data_ds = ds.Dataset(
  example_model_info["dataset_name"],
  example_model_info["config_file"],
  example_model_info["sparse_features"],
  interactive=False
)

num_features = data_ds.get_X().shape[0]

num_rows = data.shape[0]
start_date = data.idx_date.min().date().isoformat()
end_date = data.idx_date.max().date().isoformat()

y_test = example_model_info["y_test"]
probs = example_model_info["probs"]

num_bootstraps = probs.shape[1]-1 # -1 because the first column is the model-under-test
num_rows_test = len(y_test)
num_rows_train = num_rows - num_rows_test
proportion_test = num_rows_test / num_rows
```

This document contains the results of models for bleeding and ischaemia risk in heart attack patients, developed using the `{python} dataset_title` dataset. Table 1 shows a summary of the performance of the models for bleeding and ischaemia across all models and datasets.

The dataset contains `{python} num_rows` index events, defined as acute coronary syndromes (ACS) or percutaneous coronary intervention (PCI) procedures. The ACS definition has been validated to achieve at least 70% positive predictive value for identifying all myocardial infarction (MI), and distinguishing ST-segment elevation MI from non-ST-segment MI [@biobankdefinitions]. Index events span a date range from `{python} start_date` to `{python} end_date`.

Models for bleeding are trained using an outcome defintion that has been verified to identify major bleeding with positive predicted value (PPV) of 88% [@al2015identifying]. Ischaemia models are trained on a 2-point major adverse cardiac (MACE) definition including acute myocardial infarction and stroke [@hussain2018association], chosen because the components have been validated in administrative databases [@juurlink2006canadian; @kokotailo2005coding]. 


```{python}
#| label: tbl-summary
#| tbl-cap: 'Summary of model performance for bleeding and ischaemia'
#| tbl-column: body
#| output: asis

all_model_summary = []
for model, model_title in model_names.items():
  for outcome, outcome_title in outcome_names.items():
    try:
      df = get_model_summary(dataset, model, outcome)
      df.insert(0, "Model", model_title.title())
      df.insert(0, "Outcome", outcome_title.title())
      all_model_summary.append(df)
    except:
      # Failed to get summary for mobel (might not exist on disk)
      pass

try:
  summary = pd.concat(all_model_summary).reset_index(drop=True)
  print(
      summary.to_latex(
          index = False,
          position = 'h',
          caption = f"{dataset_title}",
          float_format="%.2f",
      )
  )
except:
  # This may happen if there is not table associated with the
  # dataset (not on disk)
  pass
```

In addition to the area under the receiver operating characteristic curve, models are assessed according to their stability and calibration. Instability in Table 1 is the average of the relative error between the model prediction and the predictions from other models developed on bootstrapped training sets [@riley2022stability]. It corresponds to the variation in the probability-stability plots in the document. A lower value means that bootstrapped models tend to agree with each other on the risk of adverse outcome for each patient. 

Calibration error is the average of the expected calibration error [@nixon2019measuring] across the model and its bootstrapped variants. It corresponds to how well the calibration-stability plots in this document lie along a straight line. A value closer to zero means that the risk predictions from the model tend to agree with the observed proportions of patients who do in fact have the adverse outcome.

```{python}
best_model_roc = {} # map outcome to best model for ROC AUC
best_model_calibration = {}
best_model_instability = {}
best_value_roc = {} # map outcome to best models's value for ROC AUC
best_value_calibration = {}
best_value_instability = {}
for outcome, outcome_title in outcome_names.items():
  # Calculate the best models for each criterion. These are indexes into the models
  # ordered by as in the model_names dictionary (not )
  best_for_roc = summary[summary["Outcome"] == outcome_title.title()]["ROC AUC"].idxmax()
  best_for_calibration = summary[summary["Outcome"] == outcome_title.title()]["Cal. Error"].idxmin()
  best_for_instability = summary[summary["Outcome"] == outcome_title.title()]["Instability"].idxmin()
  best_value_roc[outcome_title] = summary.iloc[best_for_roc]["ROC AUC"]
  best_value_calibration[outcome_title] = summary.iloc[best_for_calibration]["Cal. Error"]
  best_value_instability[outcome_title] = summary.iloc[best_for_instability]["Instability"]

  models = list(model_names.values())

  # The integer division by two is a hack (using the fact that the table alternates outcomes).
  # The issue is that somehow you need the lower case version of the model names, but the table
  # only has upper case versions. Really this needs a refactor of the whole stucture of this file,
  # to store references to models everywhere along with functions to obtain upper and lowercase
  # versions
  display(Markdown(
    f"For the prediction of {outcome_title} risk, "
    f"{models[best_for_roc//2]} has the highest area under the ROC curve at {best_value_roc[outcome_title]:.2f}, "
    f"{models[best_for_calibration//2]} has the best calibration (error = {best_value_calibration[outcome_title]:.2f}), and "
    f"{models[best_for_instability//2]} has the highest stability (instability = {best_value_instability[outcome_title]:.2f}).")
  )
  best_model_roc[outcome_title] = models[best_for_roc//2]
  best_model_calibration[outcome_title] = models[best_for_calibration//2]
  best_model_instability[outcome_title] = models[best_for_instability//2]

```

# Introduction

Patients who undergo percutaneous coronary (PCI) intervention to treat acute coronary syndromes (ACS) are treated by implanting a stent in a coronary artery and prescribing a course of dual-antiplatelet therapy (DAPT) or triple therapy. This medication is intended to reduce the potential for formation of clots in the coronary artery (stent thrombosis; ST), and reduces the chance of further ischaemic complications (such as another heart attack, or an ischaemic stroke). However, the chance of a severe bleeding complication is increased, which may be worse than the potential ischaemic complication. Focus is shifting to the bleeding complication, in light of advances in drug-eluting stent technology that substantially reduce the chance of ST and recurrent ACS [@capodanno2023dual]

Clinicians must balance the risk of these adverse outcomes in their daily practice. Models that attempt to assist in the assessment of bleeding and ischaemic risk have been developed [@urban2021assessing], but these are complicated by the heterogenous nature of bleeding risk in the ACS population, and the existence of a group at high bleeding risk (HBR). One defintion of high bleeding risk is the academic research consortium HBR criteria (ARC-HBR) [@urban2019defining]. The trade-off model cited above models the relative bleeding/ischaemia risk trade-off in the HBR group, and their model is intended for application to patients that fall into that category [@urban2021assessing].

Although there exist a plethora of bleeding risk scores, none have seen widespread adoption. Clinical guidelines only provide a class IIb recommendation for the use of bleeding risk scores, citing as an example the complexity of calculation and lack of validation of the ARC-HBR score as potential barriers standing in the way of its deployment [@collet20212020]. In addition, although many current bleeding risk scores are also accompanied by a mobile application or web calculator, the lack of automation of data collection and input into the programs is a likely barrier to their use in a busy clinical setting. 

To address these aspects of the problem, we consider here the development of models for bleeding and ischaemia risk based on more generic datasets of patient . Such an approach carries advantages and disadvantages. On the one hand, specific data for the identification of a particular definition of adverse outcome (such as the bleeding academic research consortium definition of major bleeding, upon which the ARC-HBR score is based) may be missing, leading to the need for an approximate outcome definition. In addition, relevant specific clinical predictors . However, the wealth of additional data present, including patient attributes derived from a primary care setting, may provide equally good predictors for bleeding and ischaemia, not considered in the typical consensus-based approach to the development of risk scores. A machine-learning approach may aid in automatically extracting these relevant features from the dataset.

If the models developed by this approach are sufficiently performative, and satisfy appropriate external validation, then it may be possible to develop a tool that automatically queries the available datasets and provides a calculation of the risk of bleeding and ischaemia to the clinician at the point of DAPT prescription. Although this step requires a full assessment of the availability and timeliness of the datasets in a clinical context, a necessary first step is the demonstration of proof-of-principle internally-validated models that can be assessed using a consistent validation framework. This report contains the results of this investigation using models trained on `{python} dataset_title`.

{{< pagebreak >}}
# Dataset and Methods

```{python}
num_bleed = (data["bleeding_al_ani_outcome"] == True).sum()
num_ami_stroke = (data["hussain_ami_stroke_outcome"] == True).sum()
```

In the full dataset (`{python} num_rows` rows), `{python} num_bleed` adverse bleeding events occurred (`{python} f"{100*num_bleed/num_rows:.1f}%"`), and `{python} num_ami_stroke` AMI or stroke events occurred (`{python} f"{100*num_ami_stroke/num_rows:.1f}%"`). The distribution of STEMI vs. NSTEMI presentation in the index events is shown below:

```{python}
import seaborn as sns
df = data.rename(columns={"dem_age": "Age at index", "idx_stemi": "STEMI"})

# Plot of the age distribution by stemi/nstemi
plt.figure(figsize=(4,3))
sns.displot(data=df, x="Age at index", hue="STEMI", bins=30).set(title='Proportion of STEMI and NSTEMI by age')
plt.show()
```

The dataset is split into a training set (`{python} num_rows_train` rows) and a testing set (`{python} num_rows_test` rows, `{python} f"{100*proportion_test:.2f}"`% of the full dataset). All models are developed on the training set, and all graphs of model performance in this report are calculated on the testing set. The dataset contains `{python} num_features` features.

## Hyperparameter tuning

Some models require choosing hyperparameters. These are optimised by fitting models for different combination of hyperparameters and assessing their performance using cross validation. In this process, the training set is split into 5 folds, each of which is successively held out as a testing set to assess the current combination of hyperparameter values. The best model is selected according to which gives the highest area under the ROC curve, and the model is refitted using the full training set. This model will be referred to as *the model* or *model-under-test* (in the context of stability analysis).

## Stability analysis

```{python}
# TODO: get number of folds from fitted object
num_folds = 5
draw_experiment_plan(num_rows, num_rows_train, num_rows_test, num_folds, num_bootstraps, fontsize=50)
```

Model stability of an internally-validated model refers to how well models developed on a similar internal population agree with each other. The intent is to estimate how well the model may generalise to external validation data. We follow the methodology outlined in [@riley2022stability] to assess the stability of the model development process, which encompasses effects due to sample size, number of predictors, and method of model fitting.

Assuming that a training set $P_0$ is used to develop a model $M_0$ (the model-under-test) using a model development process $D$ (involving steps such cross-validation and hyperparameter tuning in the training set, and validation of accuracy of model prediction in the test set), the following steps are required to assess the stability of $M_0$ [@riley2022stability]:

1. Bootstrap resample $P_0$ with replacement $M \ge 200$ times, creating $M$ new datasets $P_m$ that are all the same size as $P_0$. Here, we use `{python} num_bootstraps` bootstraps (to be increased later)
2. Apply $D$ to each $P_m$, to obtain $M$ new models $M_n$ which are all comparable with $M_0$.
3. Collect together the predictions from all Mn and compare them to the predictions from $M_0$ for each sample in the test set $T$.
4. From the data in step 3, plot instability plots such as a scatter plot of $M_0$ predictions on the x-axis and all the $M_n$ predictions on the y-axis, for each sample of $T$. In addition, plot graphs of how all the model validation metrics vary as a function of the bootstrapped models $M_n$. This includes graphs of how the ROC curves vary with bootstrapped models, and (more significantly) how the model calibration depends on the resampling.

For each model and outcome combination in this report, four figures are presented, all of which display information for the model-under-test, and also the resampled models. These are:

1. **ROC curves** for the model-under-test and the bootstrapped models, which provide an aggregate measure of how well the model balanced sensitivity and specificity over a range of different risk thresholds
2. **Model calibration** for the model-under-test and the bootstrapped models, which provides an indication of how well the predicted risks from the model align with the observed number of adverse outcomes for each of 10 risk groups (from 0% risk to 100% risk in steps of 10%)
3. **Risk prediction stability**, which directly compares the predictions of the bootstrapped models (y-axis) with the predictions from the model-under-test (x-axis) for each patient in the testing set; ideally predictions for the same patient should agree.
4. **Risk prediction distribution**, which shows how often different risks (probabilities) are predicted by the models. This is highly important in interpreting the other graphs, because some regions of risk are much more densely populated with points, which is not always obvious directly from the graph.

In addition, a meta-stability question relating to how well the different models agree with each other is also pertinent, because ideally the predictions from all the models should converge upon some (inaccessible) "ground truth" for risk of ischaemia and bleeding for each patient, if any of the models are to be trustworthy. This question is not addressed quantitatively; however, in practice, some models may be ruled out qualitatively on this criterion if their predictions or characteristics "differ considerably" from the other models. On the other hand, different models may lend each other support if they all tend to display similar behaviour.

{{< pagebreak >}}
# Models

This section contains the results for all the fitted models, including plots of the ROC curves, calibration, and probability stability.

```{python}
#| output: asis

num_models = len(model_names)
for m, (model, model_title) in enumerate(model_names.items()):
  display(Markdown(f"\n## {model_title.title()}"))
  
  bleeding_outcome = list(outcome_names)[0]
  ischaemia_outcome = list(outcome_names)[1]

  # Place a short summary about the model here
  display(Markdown("TODO short description of model here.\n"))

  plot_roc_and_calibration_2x2(dataset, model, bleeding_outcome, ischaemia_outcome)

  # Place a longer description of how the model works and behaves here
  display(Markdown("TODO summary of model performance, info about best parameters, other notes.\n"))

  display(Markdown("{{< pagebreak >}}"))
  plot_instability_2x2(dataset, model, bleeding_outcome, ischaemia_outcome)

  # Place a discussion of model stability here
  display(Markdown("TODO summary of model stability.\n"))

  display(Markdown("{{< pagebreak >}}"))

  title = f"Bleeding/Ischaemia risk tradeoff using {model_title}"
  plot_risk_tradeoff(dataset, model, bleeding_outcome, ischaemia_outcome, title)

  # Place a discussion of trade-off model here
  display(Markdown("TODO any notes about the tradeoff model here, other general notes.\n"))

  display(Markdown("{{< pagebreak >}}")) 
```

# Conclusion

In terms of an outright discrimination between bleeding or no-bleed events, `{python} f"{best_model_roc['bleeding']}"` has the best performance with a ROC AUC of `{python} f"{best_value_roc['bleeding']:.2f}"`; likewise, `{python} f"{best_model_roc['AMI or stroke']}"` has the best performance (ROC AUC `{python} f"{best_value_roc['AMI or stroke']:.2f}"`) for identifying ischaemic events.

In terms of the stability of bleeding risk predictions, which corresponds to a measure of the average uncertainty in the risk predictions themselves `{python} f"{best_model_instability['bleeding']}"` has the best performance, with an instability of `{python} f"{best_value_instability['bleeding']:.2f}"`. The most stable ischaemia model was `{python} f"{best_model_instability['AMI or stroke']}"`, with an instability of `{python} f"{best_value_instability['AMI or stroke']:.2f}"`.

The most accurately calibrated bleeding model was `{python} f"{best_model_calibration['bleeding']}"`, with a calibration error of `{python} f"{best_value_calibration['bleeding']:.2f}"`. For ischaemia, the highest calibration of `{python} f"{best_value_calibration['AMI or stroke']:.2f}"` was achieved by `{python} f"{best_model_calibration['AMI or stroke']}"`.

The model performance and validation results contained in this report can feed into any benefit-risk analysis [@iso14971] carried out as part of the development of a practical tool for bleeding and ischaemia risk trade-off prediction.

# Index and outcome definitions {.appendix}

TODO code groups used, criteria for index event, etc.

# References

::: {#refs}
:::