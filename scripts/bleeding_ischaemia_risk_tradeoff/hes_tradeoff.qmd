---
title: "Bleeding/Ischaemia Risk Trade-off Models"
project:
  output-dir: _output
execute:
  echo: false
  warning: false
format:
  html: default
  pdf:
    keep-tex: true
jupyter: python3
engine: jupyter
fig-format: png
bibliography: references.bib
csl: citation_style.csl
geometry:
  - top=15mm
  - left=15mm
  - heightrounded
---

```{python}
# Some tips on using this file
# - print() does not work for rendering markdown in code block. It will mess up
#   the order of figures and text (e.g. headings)
# - To make heading, using display(Markdown("\nHeading Name")). The newline is
#   important, and I coulnd't figure out any other way other than \n to add it.
# - In order to use inline variables like `{python} variable`, you need at least 
#   quarto 1.4. You can get the prerelease from here: https://quarto.org/docs/download/prerelease
#   (it doesn't require admin rights).
#   

import os

os.chdir("../prototypes")

import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display, Markdown

import save_datasets as ds
from stability import plot_instability
from calibration import (
    get_bootstrapped_calibration,
    plot_calibration_curves,
    plot_prediction_distribution,
)
from summarise_model import get_model_summary, plot_model_validation_2page, plot_roc_and_calibration_2x2, plot_instability_2x2
from roc import get_bootstrapped_roc, get_bootstrapped_auc, plot_roc_curves

# The dictionaries map the identifier used in filenames and in the program
# to the string that will be printed in the document. Use .title() to convert
# to a heading-like version.
dataset_name = {
    "manual_codes": "HES (manual code groups)",
    "all_codes": "HES (all codes)",
    "manual_codes_swd": "HES (manual code groups) + SWD",
}
model_names = {
    "simple_logistic_regression": "logistic regression",
    "truncsvd_logistic_regression": "truncated SVD + logistic regression",
    "simple_decision_tree": "decision tree",
}
outcome_names = {
    "bleeding_al_ani_outcome": "bleeding",
    "hussain_ami_stroke_outcome": "AMI or stroke",
}

# The generated report relates to one dataset. Pick that dataset here.
dataset = "manual_codes_swd"
dataset_title = dataset_name[dataset]
```

# Summary

```{python}
# Load any model to find which dataset was used (the script assumes that the same dataset
# was used to fit all models)
example_model_info = ds.load_fit_info(f"{dataset}_{list(model_names)[0]}_{list(outcome_names)[0]}")
dataset_path = example_model_info["dataset_path"]
data = pd.read_pickle(dataset_path)
num_rows = data.shape[0]
start_date = data.idx_date.min().date().isoformat()
end_date = data.idx_date.max().date().isoformat()
y_test = example_model_info["y_test"]
num_rows_test = len(y_test)
num_rows_train = num_rows - num_rows_test
proportion_test = num_rows_test / num_rows
```

This document contains the results of models for bleeding and ischaemia risk in heart attack patients, developed using the `{python} dataset_title` dataset. Table 1 shows a summary of the performance of the models for bleeding and ischaemia across all models and datasets. 

The dataset contains `{python} num_rows` index events, defined as acute coronary syndromes (ACS) or percutaneous coronary intervention (PCI) procedures. The ACS definition has been validated to achieve at least 70% positive predictive value for identifying all myocardial infarction (MI), and distinguishing ST-segment elevation MI from non-ST-segment MI [@biobankdefinitions]. Index events span a date range from `{python} start_date` to `{python} end_date`.

Models for bleeding are trained using an outcome defintion that has been verified to identify major bleeding with positive predicted value (PPV) of 88% [@al2015identifying]. Ischaemia models are trained on a 2-point major adverse cardiac (MACE) definition including acute myocardial infarction and stroke [@hussain2018association], chosen because the components have been validated in administrative databases [@juurlink2006canadian; @kokotailo2005coding].

```{python}
#| label: tbl-summary
#| tbl-cap: 'Summary of model performance for bleeding and ischaemia'
#| tbl-column: body
#| output: asis

all_model_summary = []
for model, model_title in model_names.items():
  for outcome, outcome_title in outcome_names.items():
    try:
      df = get_model_summary(dataset, model, outcome)
      df.insert(0, "Model", model_title.title())
      df.insert(0, "Outcome", outcome_title.title())
      all_model_summary.append(df)
    except:
      # Failed to get summary for mobel (might not exist on disk)
      pass

try:
  summary = pd.concat(all_model_summary)
  print(
      summary.to_latex(
          index = False,
          position = 'h',
          caption = f"{dataset_title}",
          float_format="%.2f",
      )
  )
except:
  # This may happen if there is not table associated with the
  # dataset (not on disk)
  pass
```

In addition to the area under the receiver operating characteristic curve, models are assessed according to their stability and calibration. Instability in Table 1 is the average of the relative error between the model prediction and the predictions from other models developed on bootstrapped training sets [@riley2022stability]. It corresponds to the variation in the probability-stability plots in the document. A lower value means that bootstrapped models tend to agree with each other on the risk of adverse outcome for each patient. 

Calibration error is the average of the expected calibration error [@nixon2019measuring] across the model and its bootstrapped variants. It corresponds to how well the calibration-stability plots in this document lie along a straight line. A value closer to zero means that the risk predictions from the model tend to agree with the observed proportions of patients who do in fact have the adverse outcome.

```{python}
for outcome, outcome_title in outcome_names.items():
  # Calculate the best models for each criterion. These are indexes into the models
  # ordered by as in the model_names dictionary (not )
  best_for_roc = summary[summary["Outcome"] == outcome_title.title()]["ROC AUC"].idxmax()
  best_for_calibration = summary[summary["Outcome"] == outcome_title.title()]["Cal. Error"].idxmin()
  best_for_stability = summary[summary["Outcome"] == outcome_title.title()]["Instability"].idxmin()

  best_roc_value = summary.iloc[best_for_roc]["ROC AUC"]
  best_calibration_error = summary.iloc[best_for_calibration]["Cal. Error"]
  best_instability = summary.iloc[best_for_stability]["Instability"]

  models = list(model_names.values())

  display(Markdown(
    f"For the prediction of {outcome_title} risk, "
    f"{models[best_for_roc]} has the highest area under the ROC curve at {best_roc_value:.3f}, "
    f"{models[best_for_calibration]} has the best calibration (error = {best_calibration_error:.3f}), and "
    f"{models[best_for_stability]} has the highest stability (instability = {best_instability:.3f}).")
  )
```

# Introduction



# Literature

Include

{{< pagebreak >}}
# Method

The dataset (`{python} num_rows` rows) is split into a training set (`{python} num_rows_train` rows) and a testing set (`{python} num_rows_test` rows, `{python} f"{100*proportion_test:.2f}"`% of the full dataset). All models are developed on the training set, and all graphs of model performance in this report are calculated on the testing set.

Some models involve automatically tuning hyperparameters. These are optimised by fitting models for each combination of hyperparameters and assessing their performance using cross validation. In this process, the training set is split into 5 folds, each of which is successively held out as a testing set to assess the current combination of hyperparameter values. The best model is selected according to which gives the highest area under the ROC curve.

{{< pagebreak >}}
# Models

```{python}
#| output: asis

# Page break before the first model section
#display(Markdown("{{< pagebreak >}}"))

  
# This is the place to put exactly one page summarising
# the dataset characteristics
display(Markdown("Dataset description here.\\"))

num_models = len(model_names)
for m, (model, model_title) in enumerate(model_names.items()):
  display(Markdown("{{< pagebreak >}}"))
  display(Markdown(f"\n## {model_title.title()}"))
  
  bleeding_outcome = list(outcome_names)[0]
  ischaemia_outcome = list(outcome_names)[1]

  # Place a short summary about the model here

  plot_roc_and_calibration_2x2(dataset, model, bleeding_outcome, ischaemia_outcome)

  # Place a longer description of how the model works and behaves here

  display(Markdown("{{< pagebreak >}}"))
  plot_instability_2x2(dataset, model, bleeding_outcome, ischaemia_outcome)

  # Place a discussion of model stability here

  # for outcome, outcome_title in outcome_names.items():
  #   #display(Markdown("{{< pagebreak >}}"))
  #   display(Markdown(f"\n### {outcome_title.title()}"))
  #   try:
  #     plot_model_validation_2page(dataset, model, outcome)
  #   except:
  #     #display(Markdown("{{< pagebreak >}}"))
  #     display(Markdown("Currently missing.\\"))
```

## Risk Trade-off Model

# Discussion

# Conclusion